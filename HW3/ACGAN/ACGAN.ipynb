{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from tqdm import tqdm  # 导入tqdm\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# 自定义数据集类\n",
    "class AnimeDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = [f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "    \n",
    "image_dir = '/home/yuchi/AI/anim'\n",
    "image_size = 64\n",
    "batch_size = 16\n",
    "\n",
    "# 图像数据预处理\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# 加载数据集\n",
    "dataset = AnimeDataset(image_dir=image_dir, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 16\n",
    "lr = 0.0001\n",
    "b1 = 0.5\n",
    "b2 = 0.999\n",
    "latent_dim = 100\n",
    "img_size = 64\n",
    "channels = 3\n",
    "\n",
    "#cuda = torch.cuda.is_available()\n",
    "cuda = False\n",
    "\n",
    "# Define weights initialization\n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "# Define Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.init_size = img_size // 4\n",
    "        self.fc = nn.Linear(latent_dim, 128 * self.init_size ** 2)\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, channels, 3, stride=1, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, noise):\n",
    "        out = self.fc(noise)\n",
    "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
    "        img = self.conv_blocks(out)\n",
    "        return img\n",
    "\n",
    "# Define Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        def block(in_filters, out_filters, bn=True):\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True)]\n",
    "            if bn:\n",
    "                layers.append(nn.BatchNorm2d(out_filters, 0.8))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(channels, 16, bn=False),\n",
    "            *block(16, 32),\n",
    "            *block(32, 64),\n",
    "            *block(64, 128),\n",
    "        )\n",
    "        ds_size = img_size // 2 ** 4\n",
    "        self.fc = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, img):\n",
    "        out = self.model(img)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        validity = self.fc(out)\n",
    "        return validity\n",
    "\n",
    "# Loss function\n",
    "adversarial_loss = torch.nn.BCELoss()\n",
    "\n",
    "# Initialize models\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "generator.apply(weights_init_normal)\n",
    "discriminator.apply(weights_init_normal)\n",
    "\n",
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    adversarial_loss.cuda()\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]: 100%|██████████| 3973/3973 [37:03<00:00,  1.79batch/s]  \n",
      "Epoch [2/10]: 100%|██████████| 3973/3973 [1:12:25<00:00,  1.09s/batch]\n",
      "Epoch [3/10]: 100%|██████████| 3973/3973 [48:55<00:00,  1.35batch/s]  \n",
      "Epoch [4/10]: 100%|██████████| 3973/3973 [48:02<00:00,  1.38batch/s] \n",
      "Epoch [5/10]: 100%|██████████| 3973/3973 [46:16<00:00,  1.43batch/s]  \n",
      "Epoch [6/10]: 100%|██████████| 3973/3973 [41:21<00:00,  1.60batch/s]\n",
      "Epoch [7/10]: 100%|██████████| 3973/3973 [48:00<00:00,  1.38batch/s] \n",
      "Epoch [8/10]: 100%|██████████| 3973/3973 [39:03<00:00,  1.70batch/s]\n",
      "Epoch [9/10]: 100%|██████████| 3973/3973 [39:03<00:00,  1.70batch/s]\n",
      "Epoch [10/10]: 100%|██████████| 3973/3973 [36:57<00:00,  1.79batch/s] \n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(n_epochs):\n",
    "    with tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{n_epochs}\", unit=\"batch\") as progress_bar:\n",
    "        for i, imgs in enumerate(progress_bar):\n",
    "            batch_size = imgs.size(0)\n",
    "            valid = Variable(torch.ones(batch_size, 1).cuda() if cuda else torch.ones(batch_size, 1))\n",
    "            fake = Variable(torch.zeros(batch_size, 1).cuda() if cuda else torch.zeros(batch_size, 1))\n",
    "            real_imgs = Variable(imgs.cuda() if cuda else imgs)\n",
    "\n",
    "            # Train Generator\n",
    "            optimizer_G.zero_grad()\n",
    "            z = Variable(torch.randn(batch_size, latent_dim).cuda() if cuda else torch.randn(batch_size, latent_dim))\n",
    "            gen_imgs = generator(z)\n",
    "            g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            # Train Discriminator\n",
    "            optimizer_D.zero_grad()\n",
    "            real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
    "            fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
    "            d_loss = (real_loss + fake_loss) / 2\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            # 更新 tqdm 描述\n",
    "            progress_bar.set_description(f\"Epoch [{epoch+1}/{n_epochs}]\")\n",
    "\n",
    "    # Save model at the end of each epoch\n",
    "    torch.save(generator.state_dict(), f'/home/yuchi/AI/ACGAN/model/generator_{epoch+1}.pth')\n",
    "    torch.save(discriminator.state_dict(), f'/home/yuchi/AI/ACGAN/model/discriminator_{epoch+1}.pth')\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [21/30]: 100%|██████████| 3973/3973 [54:37<00:00,  1.21batch/s]  \n",
      "Epoch [22/30]: 100%|██████████| 3973/3973 [46:10<00:00,  1.43batch/s] \n",
      "Epoch [23/30]: 100%|██████████| 3973/3973 [49:54<00:00,  1.33batch/s] \n",
      "Epoch [24/30]: 100%|██████████| 3973/3973 [40:12<00:00,  1.65batch/s]  \n",
      "Epoch [25/30]: 100%|██████████| 3973/3973 [1:14:58<00:00,  1.13s/batch]   \n",
      "Epoch [26/30]: 100%|██████████| 3973/3973 [1:03:13<00:00,  1.05batch/s]\n",
      "Epoch [27/30]: 100%|██████████| 3973/3973 [59:27<00:00,  1.11batch/s]  \n",
      "Epoch [28/30]: 100%|██████████| 3973/3973 [1:16:59<00:00,  1.16s/batch]\n",
      "Epoch [29/30]: 100%|██████████| 3973/3973 [55:33<00:00,  1.19batch/s]  \n",
      "Epoch [30/30]:   0%|          | 9/3973 [00:05<42:03,  1.57batch/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m gen_imgs \u001b[38;5;241m=\u001b[39m generator(z)\n\u001b[1;32m     17\u001b[0m g_loss \u001b[38;5;241m=\u001b[39m adversarial_loss(discriminator(gen_imgs), valid)\n\u001b[0;32m---> 18\u001b[0m \u001b[43mg_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m optimizer_G\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Train Discriminator\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch230/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch230/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch230/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "generator.load_state_dict(torch.load(f\"/home/yuchi/AI/ACGAN/model/generator_20.pth\"))\n",
    "discriminator.load_state_dict(torch.load(f\"/home/yuchi/AI/ACGAN/model/discriminator_20.pth\"))\n",
    "n_epochs = 30\n",
    "\n",
    "for epoch in range(20,n_epochs):\n",
    "    with tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{n_epochs}\", unit=\"batch\") as progress_bar:\n",
    "        for i, imgs in enumerate(progress_bar):\n",
    "            batch_size = imgs.size(0)\n",
    "            valid = Variable(torch.ones(batch_size, 1).cuda() if cuda else torch.ones(batch_size, 1))\n",
    "            fake = Variable(torch.zeros(batch_size, 1).cuda() if cuda else torch.zeros(batch_size, 1))\n",
    "            real_imgs = Variable(imgs.cuda() if cuda else imgs)\n",
    "\n",
    "            # Train Generator\n",
    "            optimizer_G.zero_grad()\n",
    "            z = Variable(torch.randn(batch_size, latent_dim).cuda() if cuda else torch.randn(batch_size, latent_dim))\n",
    "            gen_imgs = generator(z)\n",
    "            g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            # Train Discriminator\n",
    "            optimizer_D.zero_grad()\n",
    "            real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
    "            fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
    "            d_loss = (real_loss + fake_loss) / 2\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            # 更新 tqdm 描述\n",
    "            progress_bar.set_description(f\"Epoch [{epoch+1}/{n_epochs}]\")\n",
    "\n",
    "    # Save model at the end of each epoch\n",
    "    torch.save(generator.state_dict(), f'/home/yuchi/AI/ACGAN/model/generator_{epoch+1}.pth')\n",
    "    torch.save(discriminator.state_dict(), f'/home/yuchi/AI/ACGAN/model/discriminator_{epoch+1}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision.utils import save_image\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "# 設定參數\n",
    "latent_dim = 100  # 潛在向量的維度\n",
    "img_size = 64     # 圖片大小\n",
    "channels = 3      # 圖片通道數 (RGB)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.init_size = img_size // 4\n",
    "        self.fc = nn.Linear(latent_dim, 128 * self.init_size ** 2)\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, channels, 3, stride=1, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, noise):\n",
    "        out = self.fc(noise)\n",
    "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
    "        img = self.conv_blocks(out)\n",
    "        return img\n",
    "\n",
    "# 加載訓練好的生成器模型\n",
    "generator = Generator()\n",
    "generator.load_state_dict(torch.load(\"/home/yuchi/AI/ACGAN/model/generator_29.pth\"))  # 替換為最後保存的生成器模型檔案\n",
    "generator.eval()  # 設定為推理模式\n",
    "\n",
    "cuda = False\n",
    "device = \"cpu\"\n",
    "\n",
    "# 開始生成圖片\n",
    "num_images = 500  # 生成圖片數量\n",
    "batch_size = 16   # 每批生成圖片數量\n",
    "total_batches = num_images // batch_size\n",
    "\n",
    "with torch.no_grad():  # 禁用梯度計算以提高效率\n",
    "    img_counter = 1  # 從 1 開始命名圖片\n",
    "    for batch in range(total_batches):\n",
    "        # 隨機生成潛在向量 z\n",
    "        z = torch.randn(batch_size, latent_dim, device=device)\n",
    "        \n",
    "        # 使用生成器生成圖片\n",
    "        gen_imgs = generator(z)\n",
    "        \n",
    "        # 儲存圖片\n",
    "        for i in range(batch_size):\n",
    "            save_path = os.path.join(f\"/home/yuchi/AI/ACGAN/Result/{img_counter}.jpg\")\n",
    "            save_image(gen_imgs[i], save_path, normalize=True)\n",
    "            img_counter += 1\n",
    "\n",
    "# 處理剩餘的圖片（如果 num_images 不是 batch_size 的整數倍）\n",
    "remaining = num_images % batch_size\n",
    "if remaining > 0:\n",
    "    z = torch.randn(remaining, latent_dim, device=device)\n",
    "    gen_imgs = generator(z)\n",
    "    for i in range(remaining):\n",
    "        save_path = os.path.join(f\"/home/yuchi/AI/ACGAN/Result/{img_counter}.jpg\")\n",
    "        save_image(gen_imgs[i], save_path, normalize=True)\n",
    "        img_counter += 1\n",
    "\n",
    "!python -m pytorch_fid /home/yuchi/AI/anim /home/yuchi/AI/ACGAN/Result --batch-size 16"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch230",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
